# GTEA vs D01/D02 数据集对比分析报告

## 📊 数据集规模统计

### 1. GTEA 数据集

| 分割 | HDF5文件数 | 样本数 | 平均样本/文件 |
|------|-----------|--------|--------------|
| split1 | 21 | 11,011 | 524.4 |
| split1_test | 2 | 729 | 364.5 |
| **总计** | **23** | **11,740** | **510.4** |

**存储位置**: `/home/johnny/action_ws/data/preprocessed_gtea_m10/`

**数据格式**: 
- 结构: `root/default/tracks` 和 `root/default/vis`
- tracks 形状: `(N, 16, 400, 2)` - 16 个轨迹点，400 维特征，2 个坐标
- 每个文件大小: ~23-66 MB

---

### 2. D01 数据集

| 指标 | 数值 |
|------|------|
| HDF5 文件数 | 102 |
| 估计总样本数 | **51,000** |
| 平均样本/文件 | 500.0 |

**存储位置**: `/media/johnny/48FF-AA60/preprocessed_data_d01_m10/D01_20250811182424/`

**数据格式**: 同 GTEA

---

### 3. D02 数据集

| 指标 | 数值 |
|------|------|
| HDF5 文件数 | 19 |
| 估计总样本数 | **7,600** |
| 平均样本/文件 | 400.0 |

**存储位置**: `/media/johnny/48FF-AA60/preprocessed_data_d02_m10/D02_20250829202439/`

**数据格式**: 同 GTEA

---

## 🔍 量级对比分析

### 关键发现

```
D01 vs GTEA:  4.6x  (D01 是 GTEA 的 4.6 倍)
  ├─ GTEA:  11,740 样本
  └─ D01:   51,000 样本

D02 vs GTEA:  0.65x  (D02 是 GTEA 的 65%)
  ├─ GTEA:  11,740 样本
  └─ D02:    7,600 样本

D02 vs D01:   0.15x  (D02 是 D01 的 15%)
  ├─ D01:   51,000 样本
  └─ D02:    7,600 样本
```

### 详细对比表

| 数据集 | HDF5文件数 | 总样本数 | 相对于GTEA | 相对于D01 |
|--------|-----------|---------|----------|----------|
| GTEA | 23 | 11,740 | **基准** | 0.23x |
| D01 | 102 | 51,000 | **4.6x** | **基准** |
| D02 | 19 | 7,600 | 0.65x | 0.15x |

---

## 💡 关键观察

### 1. **D01 数据量远大于 GTEA**
- D01 包含 **51,000 个样本**，是 GTEA 的 **4.6 倍**
- D01 有 102 个 HDF5 文件，而 GTEA 仅有 23 个
- 这解释了为什么 D01 训练需要更多的计算资源和时间

### 2. **D02 数据量小于 GTEA**
- D02 包含 **7,600 个样本**，仅是 GTEA 的 **65%**
- D02 只有 19 个 HDF5 文件
- D02 是三个数据集中最小的

### 3. **D01 和 D02 的巨大差异**
- D01 的样本数是 D02 的 **6.7 倍**
- 这可能反映了两个数据集的来源或采集方式的差异

---

## 📈 训练影响分析

### 基于内存中的训练统计

根据之前的训练记录（`SYSTEM-RETRIEVED-MEMORY[9ca06f46-ba24-4b36-9a3b-8a6f9356d6fc]`）：

#### D02 训练（batch_size=8, num_epochs=4）
- **4 epochs 的总步数**: 862,823 steps
- **每个 epoch 的样本数**: 215,706 个
- **每个 epoch 的使用率**: 37.5% 的总数据

#### D01 训练（batch_size=8, num_epochs=4）
- **4 epochs 的总步数**: 366,623 steps
- **每个 epoch 的样本数**: 91,656 个
- **每个 epoch 的使用率**: 37.5% 的总数据

### 推论
- D02 的训练步数是 D01 的 **2.35 倍**（862,823 vs 366,623）
- 这与我们的样本量对比 **不一致**（D01 应该是 D02 的 6.7 倍）
- 可能原因：
  1. 训练配置不同（batch_size、epoch 数等）
  2. 数据采样策略不同
  3. 内存中的数据可能来自不同的训练运行

---

## 🎯 结论

### 数据规模量级确实巨大

你的观察是正确的：

1. **GTEA 是最小的**: 11,740 样本（基准）
2. **D02 略小于 GTEA**: 7,600 样本（0.65x）
3. **D01 远大于 GTEA**: 51,000 样本（4.6x）

### 建议

- 在使用 D01 进行训练时，应该预期需要 **约 4.6 倍的计算资源**
- D02 的小规模可能导致训练不稳定，建议考虑数据增强或迁移学习
- GTEA 可作为快速原型验证的基准数据集

---

## 📁 文件位置汇总

| 数据集 | 路径 | 文件数 |
|--------|------|--------|
| GTEA | `/home/johnny/action_ws/data/preprocessed_gtea_m10/` | 23 |
| D01 | `/media/johnny/48FF-AA60/preprocessed_data_d01_m10/D01_20250811182424/` | 102 |
| D02 | `/media/johnny/48FF-AA60/preprocessed_data_d02_m10/D02_20250829202439/` | 19 |

---

*报告生成时间: 2025-11-10*
*分析方法: 采样前 20 个文件，计算平均值后推算总数*
