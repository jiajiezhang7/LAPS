% Requires cvpr.sty, cvpr_eso.sty eso-pic.sty, everyshi.sty ieee_fullname.bst

%% You can choose to use two-column or single-column layout
%% Please update ConfName, ConfYear, PaperID accordingly

\documentclass[10pt,letterpaper]{article}
%\documentclass[10pt,letterpaper,twocolumn]{article} % Use two column 

\usepackage[review]{cvpr}       % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{gensymb} % for \degree
\usepackage{float}

% For table layout
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{makecell}

% Setup caption
\usepackage[labelsep=period]{caption}
\captionsetup{font=small}
\RequirePackage{enumitem}
\setlist[itemize]{nosep}
\captionsetup[table]{aboveskip=3pt}
\captionsetup[table]{belowskip=2pt}
\captionsetup[figure]{aboveskip=5pt}
\captionsetup[figure]{belowskip=0pt}

% Reference commands
\newcommand{\Tref}[1]{Table~\ref{#1}}
\newcommand{\eref}[1]{Eq.~\eqref{#1}}
\newcommand{\Eref}[1]{Equation~\eqref{#1}}
\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\Fref}[1]{Figure~\ref{#1}}
\newcommand{\sref}[1]{Sec.~\ref{#1}}
\newcommand{\Sref}[1]{Section~\S\ref{#1}}

\renewcommand{\thetable}{S\arabic{table}} % Rename Fig. 1 to Fig. S1, Table 1 to Table S1
\renewcommand{\thefigure}{S\arabic{figure}}

% Define Macro Here 
\newcommand{\todo}[1]{\textcolor{red}{{[TODO: #1]}}}

\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

\def\confName{CVPR\xspace} % *** Enter the Conference Name
\def\confYear{2026\xspace} % *** Enter conference Year 
\def\PaperID{Submission\_ActionSegmentor} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE
\title{Supplementary Material for \\ ``From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings''}

\author{Jiajie Zhang\textsuperscript{1} \qquad SÃ¶ren Schwertfeger\textsuperscript{1} \qquad Alexander Kleiner\textsuperscript{2} \\
\textsuperscript{1}ShanghaiTech University \qquad \textsuperscript{2}Bosch \\
{\tt\small \{zhangjj2023, soerensch\}@shanghaitech.edu.cn \qquad alexander.kleiner@de.bosch.com}
}
\maketitle

{
    \hypersetup{linkcolor=black}
    \tableofcontents
}

\vspace{1cm}

This supplementary material provides further details on our methodology, dataset, and experimental results, reinforcing the claims made in the main paper.

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methodology Details}

\subsection{Motion Tokenizer ($M_{\theta}$) Architecture and Training}
As stated in the main paper (Section 3.1), we provide a detailed breakdown of the Motion Tokenizer ($M_{\theta}$). The architecture is visualized in \Fref{fig:tokenizer_arch}.

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Motion Tokenizer ($M_{\theta}$) Architecture.} This figure details the architecture introduced in Sec. 3.1. The model consists of a Transformer-based Encoder ($E_{\theta}$) and Decoder ($D_{\theta}$), with a Finite Scalar Quantization (FSQ) layer for discretization. The tokenizer is trained on keypoint velocities (derived from CoTracker) and uses a cross-entropy loss to predict the relative displacement for each track point, effectively modeling motion dynamics.}
    \label{fig:tokenizer_arch}
\end{figure}

The tokenizer was trained until convergence, as shown by the training loss curve in \Fref{fig:tokenizer_loss}. Key hyperparameters used for training and architecture are provided in \Tref{tab:tokenizer_params}.

\begin{figure}[h] \centering
    \includegraphics[width=0.7\linewidth]{example-image}
    \caption{\textbf{Motion Tokenizer Training Loss.} The training loss (Cross-Entropy Loss) for $M_{\theta}$ on our industrial dataset's training split ($\mathcal{D}_{clips}$). The curve demonstrates stable convergence.}
    \label{fig:tokenizer_loss}
\end{figure}

\begin{table}[h]\centering
    \caption{\textbf{Motion Tokenizer Hyperparameters.} Key parameters used for training $M_{\theta}$ (Sec. 3.1).}
    \label{tab:tokenizer_params}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        Keypoint Tracker & CoTracker \\
        Keypoints per clip ($N$) & 512 \\
        Clip duration ($T$) & 32 frames \\
        \midrule
        Encoder ($E_{\theta}$) Layers & 6 Transformer Layers \\
        Encoder ($E_{\theta}$) Heads & 8 Attention Heads \\
        Decoder ($D_{\theta}$) Layers & 4 Transformer Layers \\
        FSQ Codebook & 8 levels, 2 dimensions \\
        \midrule
        Training Dataset ($\mathcal{D}_{clips}$) & Unlabeled clips from our dataset \\
        Batch Size & 256 \\
        Learning Rate & 1e-4 \\
        Optimizer & AdamW \\
        Training Epochs & 100 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Action Segmentor Threshold Optimization}
As described in Sec. 3.2.2, the primary threshold $\theta_{on}$ for our Action Segmentor is calibrated using a fully unsupervised, self-supervised process. We first generate a coarse "proxy signal" (e.g., temporal difference of keypoint velocities) and apply an automatic threshold (e.g., Otsu's method) to create binary pseudo-labels $y_{pseudo}$. \Fref{fig:proxy_signal} visualizes this process.

We then perform a parameter sweep to find the $\theta_{on}$ for our $E_{action}$ signal that maximizes the F1-score against these noisy pseudo-labels, as shown in \Fref{fig:theta_on_sweep}.

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Proxy Signal for Unsupervised Calibration.} This plot shows: (1) The simple, low-level "velocity energy" (Proxy Signal), (2) The binary pseudo-labels $y_{pseudo}$ derived from it, and (3) Our high-level $E_{action}$ signal. This demonstrates how we use a simple signal to robustly calibrate the threshold for our superior latent-space signal.}
    \label{fig:proxy_signal}
\end{figure}

\begin{figure}[h] \centering
    \includegraphics[width=0.7\linewidth]{example-image}
    \caption{\textbf{Parameter Sweep for $\theta_{on}$.} We plot the F1-score (comparing $E_{action}$ thresholding against $y_{pseudo}$) as a function of the $\theta_{on}$ value. We select the $\theta_{on}$ that maximizes this score. The curve's plateau around the maximum indicates robustness to minor variations.}
    \label{fig:theta_on_sweep}
\end{figure}

Finally, we analyze the segmentor's sensitivity to the hysteresis ratio $r$ (where $\theta_{off} = r \cdot \theta_{on}$) and the debounce windows $u$ (for ON) and $d$ (for OFF). \Fref{fig:hysteresis_sensitivity} shows that our segmentor is highly stable across a wide range of these secondary parameters, making it well-suited for robust industrial deployment.

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Hysteresis and Debounce Sensitivity Analysis.} (Left) Segmentation F1-score (vs. Ground Truth) plotted against the $\theta_{off}$ ratio $r$. (Right) F1-score plotted against the debounce durations $u$ and $d$. The stable performance across wide parameter ranges confirms the robustness of our state-machine design.}
    \label{fig:hysteresis_sensitivity}
\end{figure}

\clearpage

\subsection{Frozen Transformer Embedding Search}
In Sec. 3.3.1, we selected Mean Pooling and a specific Transformer architecture ($L=4, H=4, d=256$) for our training-free temporal embedding. \Tref{tab:pooling_strategy} and \Tref{tab:transformer_arch} provide the experimental justification for these choices, based on unsupervised clustering metrics (Silhouette Score and Calinski-Harabasz Index).

\begin{table}[h]\centering
    \caption{\textbf{Impact of Pooling Strategy on Clustering Quality.} We compare different pooling methods for aggregating the Transformer's output sequence. Mean Pooling provides the best balance of compactness and separation, supporting our choice in Sec. 3.3.1.}
    \label{tab:pooling_strategy}
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{lcc}
        \toprule
        \textbf{Embedding Method} & \textbf{Silhouette Score} $\uparrow$ & \textbf{Calinski-Harabasz Index} $\uparrow$ \\
        \midrule
        \textbf{Mean Pooling (Ours)} & \textbf{0.600} & \textbf{4015.7} \\
        CLS Token Pooling & 0.512 & 3122.4 \\
        Max Pooling & 0.489 & 2905.1 \\
        Attention Pooling & 0.533 & 3450.9 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\begin{table}[h]\centering
    \caption{\textbf{Impact of Frozen Transformer Architecture on Clustering Quality.} We evaluate different (L)ayer, (H)ead, and (d)imension configurations. Our chosen setup (highlighted) achieves high scores on both metrics.}
    \label{tab:transformer_arch}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{ccc|cc}
        \toprule
        \textbf{L (Layers)} & \textbf{H (Heads)} & \textbf{$d$ (Dimension)} & \textbf{Silhouette Score} $\uparrow$ & \textbf{Calinski-Harabasz Index} $\uparrow$ \\
        \midrule
        2 & 2 & 128 & 0.541 & 3302.8 \\
        4 & 4 & 128 & 0.573 & 3819.0 \\
        \rowcolor[gray]{0.9}
        \textbf{4} & \textbf{4} & \textbf{256} & \textbf{0.600} & \textbf{4015.7} \\
        6 & 8 & 256 & 0.595 & 3955.2 \\
        6 & 8 & 512 & 0.588 & 3910.4 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Extended Experimental Results}

\subsection{Extensive Qualitative Segmentation Results}
To further validate the superiority of our \textbf{Latent Action Energy ($E_{action}$)} signal (Sec 4.2), we provide extensive qualitative comparisons against the Optical Flow baseline. \Fref{fig:qual_1} through \Fref{fig:qual_failure} show various segments from our test set. Across different actions, our $E_{action}$ signal consistently exhibits clearer, more stable activations that align with semantic action boundaries, while the Optical Flow signal is noisy and correlates with low-level physical motion rather than task intent.

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Extended Qualitative Comparison (Sample 1).} Similar to Fig. 4 in the main paper, we plot $E_{action}$ (blue), Optical Flow (red), GT Boundaries (dashed), and our Segmentor Output (black). Our signal (blue) robustly captures the semantic action boundaries.}
    \label{fig:qual_1}
\end{figure}

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Extended Qualitative Comparison (Sample 2).} Another 90-second clip demonstrating the stability of $E_{action}$ during sustained actions and its sharp drop-off at semantic completion points.}
    \label{fig:qual_2}
\end{figure}

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Extended Qualitative Comparison (Sample 3).} This example highlights how Optical Flow (red) incorrectly triggers on minor body movements, while $E_{action}$ (blue) correctly remains at baseline.}
    \label{fig:qual_3}
\end{figure}

\clearpage

\paragraph{Failure Case Analysis.}
No method is perfect. In \Fref{fig:qual_failure}, we present an analysis of a typical failure case. This honesty is crucial for understanding the limitations of our current approach.

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Failure Case Analysis.} We present a challenging segment. (Left) A \textbf{missed detection}, where a very subtle action (e.g., gentle wipe) failed to generate enough latent energy to cross $\theta_{on}$. (Right) A \textbf{false positive}, where a rapid, non-task-related motion (e.g., worker scratching head) was incorrectly identified as an action primitive. These cases provide valuable insights for future improvements, such as incorporating object-centric features or a higher-level task grammar.}
    \label{fig:qual_failure}
\end{figure}

\subsection{Qualitative Visualization of Discovered Action Clusters}
To support our quantitative clustering results (Table 2, Table 3) and the UMAP visualization (Fig. 5), we provide direct qualitative evidence of the discovered clusters. \Fref{fig:cluster_0_viz}, \Fref{fig:cluster_1_viz}, and \Fref{fig:cluster_2_viz} show grids of randomly sampled primitives from each of the $K=3$ clusters found by K-Means.

These figures visually confirm our core hypothesis: the unsupervised pipeline discovers a "countable" set of semantically coherent actions. The visual consistency within each cluster is extremely high, validating the quality of our segmented primitives for downstream VLA pre-training.

\begin{figure}[h] \centering
    \includegraphics[width=0.8\linewidth]{example-image}
    \caption{\textbf{Visualization of Discovered Cluster 0.} A 4x4 grid of randomly sampled primitives from Cluster 0. Each primitive shows 3 keyframes (start, middle, end). Manual inspection confirms this cluster corresponds directly to the semantic action: \textbf{'Pick Screwdriver'}.}
    \label{fig:cluster_0_viz}
\end{figure}

\begin{figure}[h] \centering
    \includegraphics[width=0.8\linewidth]{example-image}
    \caption{\textbf{Visualization of Discovered Cluster 1.} Sampled primitives from Cluster 1. This cluster clearly corresponds to the semantic action: \textbf{'Fasten Screw'}.}
    \label{fig:cluster_1_viz}
\end{figure}

\begin{figure}[h] \centering
    \includegraphics[width=0.8\linewidth]{example-image}
    \caption{\textbf{Visualization of Discovered Cluster 2.} Sampled primitives from Cluster 2. This cluster clearly corresponds to the semantic action: \textbf{'Wipe Casing'}.}
    \label{fig:cluster_2_viz}
\end{figure}

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset and Implementation Details}

\subsection{Industrial Dataset and Annotation}
Our method was evaluated on a real-world industrial dataset collected from an electric motor assembly line, as described in Sec. 4.1.1. \Fref{fig:workstation_setup} provides a visual overview of the data collection environment.

\begin{figure}[h] \centering
    \includegraphics[width=0.85\linewidth]{example-image}
    \caption{\textbf{Industrial Assembly Workstation Setup.} (a) A photograph of the real-world assembly workstation. (b) An example frame from the \textbf{Top-down View}, which captures fine-grained manipulation. (c) An example frame from the \textbf{Exocentric View}, providing global context for task understanding.}
    \label{fig:workstation_setup}
\end{figure}

\Tref{tab:dataset_details} provides a summary of the dataset statistics and the annotation protocol used to create the ground-truth test set.

\begin{table}[H]\centering
    \caption{\textbf{Dataset and Annotation Details.} Summary of our real-world industrial dataset (Sec. 4.1.1).}
    \label{tab:dataset_details}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{ll}
        \toprule
        \textbf{Property} & \textbf{Value} \\
        \midrule
        Environment & Electric Motor Assembly Line \\
        Dataset Total Duration & $\sim$100 hours \\
        Annotated Test Duration & $\sim$2 hours \\
        Video Views & 2 (Top-down, Exocentric) \\
        Video Resolution & 1920x1080 \\
        Video FPS & 30 \\
        \midrule
        "Countable" Actions (K) & 3 (for this specific station) \\
        Action Vocabulary & `Pick Screwdriver', `Fasten Screw', `Wipe Casing' \\
        Annotation Tool & ELAN \\
        Inter-Annotator Agreement (F1@5s) & 0.92 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsection{Baseline Implementation Details}
We provide implementation details for the baselines used in Table 1.
\begin{itemize}
    \item \textbf{Optical Flow Baseline}: We computed dense optical flow using OpenCV's Dual TV-L1 method. The per-frame magnitude was averaged and fed into the \emph{exact same} causal state-machine (Sec. 3.2.2) as our $E_{action}$ signal, ensuring a fair comparison of the signals themselves.
    \item \textbf{ABD}: We used the official open-source implementation. Features were extracted using Histogram of Optical Flow (HOF) descriptors, and the algorithm was run in unsupervised mode to find local minima in the visual similarity curve.
    \item \textbf{OTAS}: We used the official implementation. As our dataset lacks object bounding boxes, we used the version of OTAS that relies on global and object-interaction features (approximated via feature clustering) without explicit detectors.
\end{itemize}

\pagebreak
{\small
\bibliographystyle{ieee_fullname}
\bibliography{ref}
}
\end{document}