#!/usr/bin/env python3
"""
Diagnose HDF5 files generated by GTEA preprocessing.
- List groups and datasets
- Print dtype/shape/compression metadata
- Attempt to read a small slice from datasets to trigger potential I/O/type errors

Usage:
  python tools/diagnose_gtea_hdf5.py --dir /path/to/hdf5_root \
      --files S2_Cheese_C1.hdf5,S1_Coffee_C1.hdf5  # optional specific files
  # or pick first N files alphabetically
  python tools/diagnose_gtea_hdf5.py --dir /path/to/hdf5_root --pick 3
"""
from __future__ import annotations

import argparse
import os
import sys
from pathlib import Path
from typing import List

import h5py
from h5py import h5t


def fmt(obj):
    try:
        return str(obj)
    except Exception:
        try:
            return repr(obj)
        except Exception:
            return f"<unrepr {type(obj).__name__}>"


def inspect_dataset(dset: h5py.Dataset) -> List[str]:
    lines = []
    try:
        lines.append(f"  - path: {dset.name}")
        # Shape can usually be read even if dtype mapping fails
        try:
            lines.append(f"    shape: {dset.shape}")
        except Exception as e:
            lines.append(f"    shape_error: {type(e).__name__}: {e}")
        # High-level dtype may trigger mapping error; guard separately
        try:
            lines.append(f"    dtype: {dset.dtype}")
        except Exception as e:
            lines.append(f"    dtype_error: {type(e).__name__}: {e}")
        # Low-level HDF5 type info (should not require NumPy dtype mapping)
        try:
            t = dset.id.get_type()
            cls = t.get_class()
            size = t.get_size()
            lines.append(f"    h5t_class: {cls}")
            lines.append(f"    h5t_size: {size}")
            try:
                order = t.get_order()
                lines.append(f"    h5t_order: {order}")
            except Exception:
                pass
        except Exception as e:
            lines.append(f"    h5t_error: {type(e).__name__}: {e}")
        # Storage filters
        try:
            lines.append(f"    chunks: {fmt(dset.chunks)}")
            lines.append(f"    compression: {fmt(dset.compression)}")
            lines.append(f"    compression_opts: {fmt(dset.compression_opts)}")
            lines.append(f"    shuffle: {fmt(getattr(dset, 'shuffle', None))}")
            lines.append(f"    fletcher32: {fmt(getattr(dset, 'fletcher32', None))}")
            lines.append(f"    scaleoffset: {fmt(getattr(dset, 'scaleoffset', None))}")
        except Exception as e:
            lines.append(f"    storage_props_error: {type(e).__name__}: {e}")
        # Try small reads
        try:
            _ = dset[0]
            lines.append("    read_first: OK")
        except Exception as e:
            lines.append(f"    read_first: ERROR: {type(e).__name__}: {e}")
        # Try small slice
        try:
            if len(dset.shape) >= 1 and dset.shape[0] > 1:
                sl = dset[0:2]
            else:
                sl = dset[...]
            _ = (sl.shape, getattr(sl, 'dtype', None))
            lines.append("    read_slice: OK")
        except Exception as e:
            lines.append(f"    read_slice: ERROR: {type(e).__name__}: {e}")
    except Exception as e:
        lines.append(f"  - dataset_inspect_error: {type(e).__name__}: {e}")
    return lines


def inspect_file(path: Path) -> List[str]:
    lines: List[str] = []
    lines.append(f"\n=== Inspect: {path} ===")
    if not path.exists():
        lines.append("  (missing)")
        return lines
    try:
        with h5py.File(path, 'r') as f:
            # list groups
            try:
                keys_lvl0 = list(f.keys())
            except Exception as e:
                lines.append(f"  error_list_keys: {type(e).__name__}: {e}")
                return lines
            lines.append(f"  keys@/: {keys_lvl0}")
            # check root/default group
            root = f.get('root')
            if root is None:
                lines.append("  (no 'root' group)")
                return lines
            default = root.get('default')
            if default is None:
                lines.append("  (no 'root/default' group)")
                return lines
            # group attrs (height/width)
            try:
                h = default.attrs.get('height')
                w = default.attrs.get('width')
                lines.append(f"  attrs@root/default: height={fmt(h)}, width={fmt(w)}")
            except Exception as e:
                lines.append(f"  attrs@root/default: ERROR: {type(e).__name__}: {e}")
            # datasets
            for name in ('tracks', 'vis'):
                ds = default.get(name)
                if ds is None:
                    lines.append(f"  (no dataset '{name}' under root/default)")
                else:
                    lines.extend(inspect_dataset(ds))
    except Exception as e:
        lines.append(f"  open_file_error: {type(e).__name__}: {e}")
    return lines


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--dir', required=True, help='Directory containing HDF5 files')
    ap.add_argument('--files', type=str, default='', help='Comma-separated list of specific .hdf5 files to inspect (relative to --dir)')
    ap.add_argument('--pick', type=int, default=3, help='Pick first N files alphabetically when --files is empty')
    args = ap.parse_args()

    root = Path(args.dir)
    if not root.exists():
        print(f"[ERR] Directory not found: {root}", file=sys.stderr)
        sys.exit(2)

    targets: List[Path] = []
    if args.files:
        for nm in args.files.split(','):
            nm = nm.strip()
            if not nm:
                continue
            p = root / nm
            targets.append(p)
    else:
        all_files = sorted([p for p in root.rglob('*.hdf5') if p.is_file()])
        if args.pick > 0:
            targets = all_files[: args.pick]
        else:
            targets = all_files

    if not targets:
        print(f"[WARN] No HDF5 files selected under {root}")
        return

    print(f"Inspecting {len(targets)} files under {root} ...")
    for p in targets:
        for line in inspect_file(p):
            print(line)


if __name__ == '__main__':
    main()

